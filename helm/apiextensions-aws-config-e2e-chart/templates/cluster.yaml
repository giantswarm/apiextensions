apiVersion: "provider.giantswarm.io/v1alpha1"
kind: AWSConfig
metadata:
  name: {{.Values.clusterName}}
spec:
  cluster:
    version: "{{.Values.clusterVersion}}"

    id: "{{.Values.clusterName}}"

    customer:
      id: "example-customer"

    docker:
      daemon:
        cidr: "172.17.0.1/16"
        extraArgs: "--log-opt max-size=25m --log-opt max-file=2 --log-opt labels=io.kubernetes.container.hash,io.kubernetes.container.name,io.kubernetes.pod.name,io.kubernetes.pod.namespace,io.kubernetes.pod.uid"

    etcd:
      domain: "etcd.{{.Values.clusterName}}.k8s.{{.Values.commonDomain}}"
      prefix: "giantswarm.io"
      port: 2379

    calico:
      subnet: "192.168.0.0"
      cidr: 16
      mtu: 1500
      domain: "calico.{{.Values.clusterName}}.k8s.{{.Values.commonDomain}}"

    kubernetes:
      api:
        domain: "api.{{.Values.clusterName}}.k8s.{{.Values.commonDomain}}"
        insecurePort: 8080
        ip: "172.31.0.1"
        securePort: 443
        clusterIPRange: "172.31.0.0/24"
        altNames: "kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster.local"
      cloudProvider: "aws"
      dns:
        ip: "172.31.0.10"
      domain: "cluster.local"
      hyperkube:
        docker:
          image: "quay.io/giantswarm/hyperkube:v1.7.5_coreos.0"
      ingressController:
        docker:
          image: "quay.io/giantswarm/nginx-ingress-controller:0.9.0-beta.11"
        insecurePort: 30010
        securePort: 30011
        domain: "ingress.{{.Values.clusterName}}.k8s.{{.Values.commonDomain}}"
        wildcardDomain: "*.{{.Values.clusterName}}.k8s.{{.Values.commonDomain}}"
      kubectl:
        docker:
          image: "quay.io/giantswarm/docker-kubectl:f51f93c30d27927d2b33122994c0929b3e6f2432"
      kubelet:
        altNames: "kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster.local"
        domain: "worker.{{.Values.clusterName}}.k8s.{{.Values.commonDomain}}"
        labels: "giantswarm.io/provider=aws,aws-operator.giantswarm.io/version={{ .Values.versionBundleVersion }}"
        port: 10250
      networkSetup:
        docker:
          image: "quay.io/giantswarm/k8s-setup-network-environment:1f4ffc52095ac368847ce3428ea99b257003d9b9"
      ssh:
        userList:
        - name: "{{.Values.sshUser}}"
          publicKey: "{{.Values.sshPublicKey}}"
    masters:
    - id: "master-1"

    workers:
    - id: "worker-1"
    - id: "worker-2"

  aws:
    region: "{{.Values.aws.region}}"
    az: "{{.Values.aws.az}}"

    # Deprecated since aws-operator v12 resources.
    api:
      hostedZones: "{{.Values.aws.apiHostedZone}}"
      elb:
        idleTimeoutSeconds: 1200

    credentialSecret:
      name: "{{.Values.aws.credentialName}}"
      namespace: "{{.Values.aws.credentialNamespace}}"

    # Deprecated since aws-operator v12 resources.
    etcd:
      hostedZones: "{{.Values.aws.apiHostedZone}}"
      elb:
        idleTimeoutSeconds: 3600

    hostedZones:
      api:
        name: "{{.Values.commonDomain}}"
      etcd:
        name: "{{.Values.commonDomain}}"
      ingress:
        name: "{{.Values.commonDomain}}"

    # Deprecated since aws-operator v12 resources.
    ingress:
      hostedZones: "{{.Values.aws.apiHostedZone}}"
      elb:
        idleTimeoutSeconds: 60

    vpc:
      cidr: "{{.Values.aws.networkCIDR}}"
      privateSubnetCidr: "{{.Values.aws.privateSubnetCIDR}}"
      publicSubnetCidr: "{{.Values.aws.publicSubnetCIDR}}"
      routeTableNames:
      - {{.Values.aws.routeTable0}}
      - {{.Values.aws.routeTable1}}
      peerId: "{{.Values.aws.vpcPeerId}}"

    masters:
    - imageID: "{{.Values.aws.ami}}"
      instanceType: "{{.Values.aws.instanceTypeMaster}}"

    workers:
    - imageID: "{{.Values.aws.ami}}"
      instanceType: "{{.Values.aws.instanceTypeWorker}}"
    - imageID: "{{.Values.aws.ami}}"
      instanceType: "{{.Values.aws.instanceTypeWorker}}"

  guest:
    update:
      enabled: {{.Values.updateEnabled}}

  versionBundle:
    version: "{{.Values.versionBundleVersion}}"
